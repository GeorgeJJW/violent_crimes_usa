---
title: EDA
output: github_document
---

This is a exploratory data analysis to explore the [Marshall Violent Crime](https://github.com/themarshallproject/city-crime) dataset.

```{r}
# load libraries
suppressPackageStartupMessages(library(tidyverse))
library(ggmap)
register_google(key = "")
```

```{r}
# load data
crime <- read_csv("../data/ucr_crime_1975_2015.csv")
latlon <- read_csv("../data/city_latlon.csv")
```



```{r}
# some information for the usage scenario
crime %>% 
  filter(year == 2015, department_name == "Denver")
```

```{r}
crime %>% 
  filter(year == 2015, department_name == "Kansas City, Mo.")
```


```{r}
# size of data
dim(crime )
```

There are 2,829 entries in the raw dataset.

```{r}
# explore department_name (city name) variable
crime$department_name %>%
  unique()
```

There are some potential issues with this variable:

- some police departments oversee counties as well, for the purpose of this app, we only want to display information about cities.

- the variable value "National" is not a meaningful entry for our app.

```{r}
# count the number of problematic city names
bad_names <- crime$department_name %>%
  unique() %>% 
  str_subset(pattern = "County|National")
bad_names %>% print()
```

```{r}
bad_names %>% length()
```

There are 9 potentially problematic city names for our application.

```{r}
# explore incomplete entries
# identify annual reports with less than 12 months reported
crime %>% 
  filter(months_reported < 12) %>% 
  select(ORI, months_reported)
```

There are 53 annual reports with less than 12 months reported.

```{r}
# tentative attempt at removing problematic entries
# at worst case, we are removing all incomplete annual reports
# at worst case, we are removing all potentially problematic city names
crime_clean <- crime %>% 
  filter(months_reported == 12, !department_name %in% bad_names) %>% 
  separate(department_name, into = c("city", "state"), sep = ",") %>% 
  mutate(city = str_replace(city, "Charlotte-Mecklenburg", "Charlotte")) %>%
  mutate(city = str_replace(city, "New York City", "New York")) %>%  
  left_join(latlon, by = c("city" = "locality")) %>%
  separate(address, into = c("cityname", "statecode", "country"), sep = ",") %>% 
  mutate(state = str_to_upper(statecode)) %>% 
  unite("department_name", c(city, state), sep = ",") %>% 
  select(department_name, year, total_pop, violent_per_100k, homs_per_100k, rape_per_100k, rob_per_100k, agg_ass_per_100k, lon, lat) %>%
  filter(!department_name %in% overlaps) %>% 
  write_csv("../data/crime_clean.csv")
```

At worst case, we are left with 2,348 seemingly valid entries.

The following commands are for fetching lat/lon for each city names using Google's API.

```{r}
# find overlapping cities
crime_clean %>% 
  select(department_name, lat, lon) %>% 
  unique() %>% 
  arrange(lon)
```

```{r}
# identify cities with major overlaps to other cities
# consider dropping these cities
overlaps <- c("Fort Worth, TX", 
              "Arlington, TX", 
              "Mesa, AZ", 
              "Long Beach, CA", 
              "Oakland, CA",
              "Aurora, CO", 
              "Newark, NJ",
              "Baltimore, MD",
              "Honolulu, HI")
```


```{r}
# add lat/lon coordinates to cities
(city_latlon <- crime_clean$department_name %>% 
  unique() %>% 
  geocode(output = "more", source = "google"))
```

```{r}
# save output
city_latlon %>%
  write_csv(path = "../data/city_latlon.csv")
```

